import streamlit as st
import tempfile
import json
import os
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_cohere import ChatCohere, CohereEmbeddings
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from evaluate import load as load_metric
import pandas as pd

# --- CSS Styling ---
st.markdown("""
    <style>
    .summary-box {
        background-color: #f9f9f9;
        border-radius: 10px;
        padding: 15px;
        margin-bottom: 20px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        font-family: 'Segoe UI', sans-serif;
    }
    .metric-table {
        font-size: 16px;
    }
    </style>
""", unsafe_allow_html=True)

# --- Load environment variables ---
load_dotenv()

# --- Session State Initialization ---
for key in ["results", "evaluation_results", "best_model", "best_model_explanation", "processed"]:
    if key not in st.session_state:
        st.session_state[key] = None if key != "processed" else False

# --- LLM Setup ---
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

llms = {
    "Cohere": ChatCohere(cohere_api_key=COHERE_API_KEY, model="command-a-03-2025", temperature=0),
    "LLaMA": ChatGroq(model="llama-3.1-8b-instant", groq_api_key=GROQ_API_KEY, temperature=0),
    "GPT-4": ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-4")
}

embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY, model="embed-v4.0")

# --- Functions ---

def get_conversational_chain(llm):
    prompt_template = """You are a highly skilled academic summarizer with extensive experience in distilling complex research papers into clear, concise summaries that highlight key findings and implications.

The summary should include a short introduction, the key findings, and their importance in a non-technical way.

Document Context:
{context}"""
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = prompt | llm | StrOutputParser()
    return chain

def run_summarization_with_chain(llm, docs, query):
    chain = get_conversational_chain(llm)
    return chain.invoke({"context": docs, "question": query})

def evaluate_summary(prediction, reference):
    rouge = load_metric("rouge")
    bleu = load_metric("bleu")
    meteor = load_metric("meteor")

    rouge_result = rouge.compute(predictions=[prediction], references=[reference])
    bleu_result = bleu.compute(predictions=[prediction], references=[reference])
    meteor_result = meteor.compute(predictions=[prediction], references=[reference])

    return {
        "ROUGE": rouge_result,
        "BLEU": bleu_result,
        "METEOR": meteor_result
    }

def format_metrics_for_display(evaluation_results):
    return pd.DataFrame([
        {
            "Model": name,
            "ROUGE-1": metrics["ROUGE"]["rouge1"],
            "ROUGE-L": metrics["ROUGE"]["rougeL"],
            "BLEU": metrics["BLEU"]["bleu"],
            "METEOR": metrics["METEOR"]["meteor"],
        }
        for name, metrics in evaluation_results.items()
    ])

def interpret_metrics_with_llm(metrics_dict, llm):
    formatted_scores = ""
    for model, metrics in metrics_dict.items():
        rouge = metrics["ROUGE"]["rouge1"]
        bleu = metrics["BLEU"]["bleu"]
        meteor = metrics["METEOR"]["meteor"]
        formatted_scores += (
            f"{model}:\n"
            f"  ROUGE-1: {rouge:.3f}\n"
            f"  BLEU: {bleu:.3f}\n"
            f"  METEOR: {meteor:.3f}\n\n"
        )

    prompt = f"""
Here are the evaluation scores (ROUGE-1, BLEU, METEOR) for summaries generated by different language models compared against GPT-4:

{formatted_scores}

Please analyze the scores and tell which model performed best overall. Your answer should be short (2-3 lines), clear, and reference the model name directly.
"""
    response = llm.invoke(prompt)
    return response.content.strip()

# --- UI ---
st.title("üìÑ PDF Summarizer with Multiple LLMs")
st.subheader("Upload a PDF and compare summaries from various models")

uploaded_file = st.file_uploader("Upload your PDF", type="pdf")

if uploaded_file is not None:
    if st.button("Summarize PDF"):
        st.session_state.processed = False

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_path = tmp_file.name

        with st.spinner("üîç Processing PDF..."):
            loader = PyPDFLoader(tmp_path)
            raw_documents = loader.load()
            text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=300)
            documents = text_splitter.split_documents(raw_documents)

            db = Chroma.from_documents(documents, embeddings)
            retriever = db.as_retriever()
            query = "Can you summarise the whole document for me?"
            input_docs = retriever.invoke(query)

            results = {}
            for name, model in llms.items():
                results[name] = run_summarization_with_chain(model, input_docs, query)

            st.session_state.results = results

            gpt_summary = results.get("GPT-4")
            evaluation_results = {
                name: evaluate_summary(summary, gpt_summary)
                for name, summary in results.items() if name != "GPT-4"
            }

            st.session_state.evaluation_results = evaluation_results
            best_model_explanation = interpret_metrics_with_llm(evaluation_results, llms["LLaMA"])
            st.session_state.best_model_explanation = best_model_explanation

            # Compute best model (highest avg score)
            def average(metrics):
                return (metrics["ROUGE"]["rouge1"] + metrics["BLEU"]["bleu"] + metrics["METEOR"]["meteor"]) / 3

            best_model = max(evaluation_results.items(), key=lambda item: average(item[1]))[0]
            st.session_state.best_model = best_model
            st.session_state.processed = True

            # Save summaries to JSON
            os.makedirs("summaries", exist_ok=True)
            filename = f"summaries/summaries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, "w") as f:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "query": query,
                    "summaries": results
                }, f, indent=4)

            st.info(f"‚úÖ Summaries saved to: {filename}")

# --- Results ---
if st.session_state.processed and st.session_state.results:
    if st.session_state.best_model:
        st.success(f"üèÜ Best Performing Model: {st.session_state.best_model}")
    if st.session_state.best_model_explanation:
        st.markdown("### üß† Best Model Explanation (LLaMA)")
        st.info(st.session_state.best_model_explanation)

    st.markdown("## üìä Evaluation Metrics (vs GPT-4)")
    if st.session_state.evaluation_results:
        st.dataframe(format_metrics_for_display(st.session_state.evaluation_results).style.format(precision=3), use_container_width=True)

    st.markdown("## üìö Summaries")
    for name, summary in st.session_state.results.items():
        with st.expander(f"üìù Summary by {name}", expanded=(name == st.session_state.best_model)):
            st.markdown(f'<div class="summary-box">{summary}</div>', unsafe_allow_html=True)
